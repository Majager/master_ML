{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob \n",
    "import os\n",
    "import json\n",
    "from matplotlib.colors import LogNorm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_folder_path = \"N:\\\\durable\\\\sound-and-ecg\\\\2024-10-Maja-restructured\\\\Data\"\n",
    "root_folder_path = \"C:\\\\Users\\\\MajaE\\\\src\\\\repos\\\\master_ML\\\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_parameters(test_file):\n",
    "    true,predictions,predictions_proba,test_recording_ids,segment_parameters =[],[],[],[],[]\n",
    "    with open(test_file, 'rb') as handle:\n",
    "        true,predictions,predictions_proba,test_recording_ids, segment_parameters = pickle.load(handle)\n",
    "    return true,predictions,predictions_proba,test_recording_ids,segment_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true,predictions):\n",
    "    accuracy = accuracy_score(true, predictions) # ((TN + TP) / (TN + FN + TP + FP))\n",
    "    f1 = f1_score(true,predictions,zero_division=np.nan,average='macro')\n",
    "    return accuracy,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_avg_performance(folder_path):\n",
    "    test_files = glob.glob(os.path.join(folder_path,'*.pickle'))\n",
    "    true, predictions, predictions_proba, ids, segment_parameters = [], [], [], [], []\n",
    "    for test_file in test_files:\n",
    "        true_subject, predictions_subject,predictions_proba_subject,test_recording_ids,segment_parameters_subject = extract_test_parameters(test_file)\n",
    "        true.extend(true_subject)\n",
    "        predictions.extend(predictions_subject)\n",
    "        predictions_proba.extend(predictions_proba_subject)\n",
    "        ids.extend(test_recording_ids)\n",
    "        segment_parameters.extend([segment_parameters_subject])\n",
    "    accuracy_vector, f1_vector = [], []\n",
    "    for subject_idx in range(len(true)):\n",
    "        accuracy,f1 = calculate_metrics(true[subject_idx],predictions[subject_idx])\n",
    "        accuracy_vector.append(accuracy)\n",
    "        f1_vector.append(f1)\n",
    "    accuracy = np.nanmean(accuracy_vector)\n",
    "    f1 = np.nanmean(f1_vector)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_performance(root_folder_path):\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    run_folder_path = root_folder_path\n",
    "    for run in os.listdir(root_folder_path):\n",
    "        run_accuracy = []\n",
    "        run_f1 = []\n",
    "        run_folder_path = os.path.join(root_folder_path,run)\n",
    "        data_folder_path = os.path.join(run_folder_path,\"data\")\n",
    "        for iteration in os.listdir(data_folder_path):\n",
    "            iteration_path = os.path.join(data_folder_path,iteration)\n",
    "            i_accuracy, i_f1 = find_avg_performance(iteration_path)\n",
    "            run_accuracy.append(i_accuracy)\n",
    "            run_f1.append(i_f1)\n",
    "        accuracy.append(run_accuracy)\n",
    "        f1.append(run_f1)\n",
    "    \n",
    "    avg_accuracy = []\n",
    "    avg_f1 = []\n",
    "    std_accuracy = []\n",
    "    std_f1 = []\n",
    "    n_runs = len(accuracy)\n",
    "    n_features = len(accuracy[0])\n",
    "    for i in range (n_features):\n",
    "        acc_sum = 0\n",
    "        f1_sum = 0\n",
    "        for j in range (n_runs):\n",
    "            acc_sum += accuracy[j][i]\n",
    "            f1_sum += f1[j][i]\n",
    "        mean_acc = acc_sum/n_runs\n",
    "        mean_f1 = f1_sum/n_runs\n",
    "        avg_accuracy.append(mean_acc)\n",
    "        avg_f1.append(mean_f1)\n",
    "\n",
    "        variance_acc = sum((accuracy[j][i] - mean_acc) ** 2 for j in range(n_runs)) / n_runs\n",
    "        std_acc = math.sqrt(variance_acc)\n",
    "        std_accuracy.append(std_acc)\n",
    "        variance_f1 = sum((f1[j][i] - mean_f1) ** 2 for j in range(n_runs)) / n_runs\n",
    "        std_feature_f1 = math.sqrt(variance_f1)\n",
    "        std_f1.append(std_feature_f1)\n",
    "\n",
    "    parameters_path = f\"{run_folder_path}\\\\parameters.json\"\n",
    "    test_values = []\n",
    "    with open(parameters_path, 'r') as file:\n",
    "        meta_data = json.load(file)\n",
    "        test_values = meta_data[\"test_values\"]\n",
    "    \n",
    "    return avg_accuracy, avg_f1, std_accuracy, std_f1, test_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_curve(selection_algorithms,label):\n",
    "    accuracies, f1_scores, std_accuracies, std_f1_scores, n_features_vec = [], [], [], [], []\n",
    "    for selection_algorithm in selection_algorithms:\n",
    "        accuracy, f1, std_accuracy, std_f1, n_features = feature_selection_performance(selection_algorithm)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        n_features_vec.append(n_features) \n",
    "        std_accuracies.append(std_accuracy)\n",
    "        std_f1_scores.append(std_f1)\n",
    "\n",
    "    plt.figure()\n",
    "    for idx in range (len(accuracies)):\n",
    "        acc = np.array(accuracies[idx])\n",
    "        std = np.array(std_accuracies[idx])\n",
    "        n_features = np.array(n_features_vec[idx])\n",
    "        line, = plt.plot(n_features,acc,label=selection_algorithms[idx],marker='o',markersize=6)\n",
    "        plt.fill_between(n_features,acc-std,acc+std,color=line.get_color(), alpha = 0.2)\n",
    "    plt.xlabel('Number of selected features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Performance of {label}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    for idx in range (len(f1_scores)):\n",
    "        f1 = np.array(f1_scores[idx])\n",
    "        std = np.array(std_f1_scores[idx])\n",
    "        n_features = np.array(n_features_vec[idx])\n",
    "        line, = plt.plot(n_features,f1,label=selection_algorithms[idx],marker='o',markersize=6)\n",
    "        plt.fill_between(n_features,f1-std,f1+std,color=line.get_color(), alpha = 0.2)\n",
    "    plt.xlabel('Number of selected features')\n",
    "    plt.ylabel('F1')\n",
    "    plt.title(f'Performance of {label}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['No meal','Meal']\n",
    "#selection_algorithms = [\"mutual_information_LDA\",\"sfs_LDA\",\"RFE_LDA\"]\n",
    "#selection_algorithms = [\"mutual_information_HMM\",\"sfs_HMM\"]\n",
    "#feature_selection_curve(selection_algorithms,'HMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  7  8  9 12 14 15 18 19 21 23 24 26 28 30 32 34 37 40 41 42 44]\n"
     ]
    }
   ],
   "source": [
    "importance_mutual_information, importance_sfs = [],[]\n",
    "# Extract features from previous calculations\n",
    "with open(f'..\\\\feature_selection_HMM.pickle', 'rb') as handle:\n",
    "    importance_mutual_information, importance_sfs = pickle.load(handle)\n",
    "\n",
    "values = list(range(1,23+1))\n",
    "indices_list = np.where(np.isin(importance_sfs,values))[0]\n",
    "print(indices_list)\n",
    "\n",
    "# with open(f'..\\\\selected_features_HMM.pickle','wb') as handle:\n",
    "#     pickle.dump([indices_list],handle,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  5  8 10 13 14 19 20 28 42]\n"
     ]
    }
   ],
   "source": [
    "selected_features_idx = []\n",
    "with open(f'..\\\\selected_features_LDA.pickle','rb') as handle:\n",
    "    selected_features_idx = pickle.load(handle)[0]\n",
    "\n",
    "print(selected_features_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
